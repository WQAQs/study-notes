# 2.**感知机**
**定义**：**感知机**（perceptron）是二分类线性分类模型，其输入为实例的特征向量，输出为实例的类别，取$+1$和$-1$二值，属于判别模型。
给定一个**数据集**
$$
T=\{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}
$$ 其中，$x_i\in\mathcal{X}=R^n,y_i\in \mathcal{Y}=\{+1,-1\},i=1,2,\cdots,N$

- **补充知识**
  - **$\ell_p$范数**
  $\ell_p$范数表示一组范数：
  $$\ell_p=||x||_p=\sqrt [p]{\sum_{i=1}^{n}x_i^p},x=(x_1,x_2,\cdots,x_n)$$ **常见范数**：
  （1）$\ell_0$范数：即表示向量$x$中非零元素的个数。$$\ell_0=||x||_0$$
  （2）$\ell_1$范数：即表示向量$x$中元素的绝对值之和。$$\ell_1=||x||_1=\sum_{i=1}^{n}|x_i|$$
  （3）$\ell_2$范数：即表示向量$x$中元素的平方和再开方（坐标点到原点的欧几里和距离）。$$\ell_2=||x||_2=\sqrt [2]{\sum_{i=1}^{n}x_i^2}$$**注意**:$||x||$表示$\ell_2$范数，只是把2省略了。

  - **超平面**
  超平面是比所研究的环境空间低一个维度的子空间。例如二维空间平面，超平面就是一条直线。三维空间，超平面就是一个平面。
  **超平面方程**：$w\cdot x+b=0$,表示二维空间中的一个超平面，即对应一条直线。
  **点与超平面的关系**：
  $$
  x在平面S的\begin{cases}
  正面,w^T\cdot x+b>0\\
  平面上,w^T\cdot x+b=0\\
  反面,w^T\cdot x+b<0\\
  \end{cases}
  $$ **注意**：$w^T\cdot x+b$的值正得越大, 代表点在平面的正向且与平面的距离越远。 反之, 它的值负得越大, 代表点在平面的反向且与平面的距离越远。
  **点到超平面的距离**：$$\frac{w}{||w||}\cdot x_i+\frac{b}{||w||}$$
  - **平面**
  **平面方程**：方程$ax+by+cz+d=0$表示三维空间内的一个平面$S$。在平面$S$上的点$(x_i,y_i,z_i)$满足$ax_i+by_i+cz_i+d=0$
  **平面的法向量**：$(a,b,c)$
  **点到平面的距离**：$$d(x_i,y_i,z_i)=\frac{|ax_i+by_i+cz_i+d|}{\sqrt{a^2+b^2+c^2}}$$
## 2.1.**感知机模型**

$$
f(x)=sign(w\cdot x+b)
$$
其中，$w\in R^n$叫权值（weight）或权值向量（weight vector），$b\in R$叫作偏置（bias），$w\cdot x$表示$w$和$x$的内积，$sign$是符号函数，即
$$
sign(x)=\begin{cases}
+1,x\geq 0\\
-1,x\leq 0  
\end{cases}
$$
- **分离超平面**
  线性方程
  $$
  w\cdot x+b=0
  $$ 对应特征空间$R^n$中的一个超平面$S$,其中$w$是**超平面的法向量**，$b$是**超平面的截距**。这个朝平面将特征空间划分为两个部分，位于两部分的点（特征向量）分别被分为正、负两类。因此超平面$S$被称为**分离超平面（separating hyperplane）**。
- **数据集的线性可分性**
  如果存在某个超平面$w\cdot x+b=0$能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i=+1$的实例，都有$w\cdot x_i+b>0$;对于所有的$y_i=-1$的实例，都有$w\cdot x_i+b<0$，则称数据集$T$为线性可分数据集（linearly separable data set）
- **函数间隔和几何间隔**
  样本点$(x_i,y_i)$与超平面$w\cdot x+b=0$之间的
  (1)**函数间隔**：$$\gamma_i=y_i(w\cdot x_i+b)$$ 但该定义存在问题，当$w$和$b$同时缩小或放大$m$倍后，超平面并没有变化，但函数间隔却变了。所以，作规范化$\frac{w}{||w||}$,即相当于$||w'||=1$，这时候求得的即为几何间隔。
  (2)**几何间隔**：$$\gamma_i=y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||}) $$实际上几何间隔就是点到超平面的距离。

## 2.2.**感知机学习策略**
- **损失函数**
  - 损失函数采用输入空间$R^n$中任一**点$x_0$到超平面$S$的距离**：
  $$
  \frac{1}{||w||}|w\cdot x_0+b|
  $$ 其中$||w||$是$w$的$L_2$范数。
  - 对于**误分类的数据**来说：
  $$-y_i(w\cdot x_i+b)>0$$
  - 所有误分类点到超平面$S$的**总距离**为：
  $$ -\frac{1}{||w||}\sum_{x_i\in M}y_i(w\cdot x_0+b)$$
  - **感知机的损失函数**：
  不考虑上面总距离中的$\frac{1}{||w||}$,就得到损失函数
  $$ -\sum_{x_i\in M}y_i(w\cdot x_0+b)$$
  **注意**：感知机中损失函数1/||w||为什么可以不考虑?
  （1）实际上**感知机的策略即是损失函数使用函数间隔**。因为感知机处理的是二分类任务，是误分类驱动的，做规范化的部分$\frac{1}{||w||}$并不影响正负的判断。
  （2）感知机的二分类目标就是分类尽量正确就行，而SVM希望得到效果最好的超平面。
## 2.3.**感知机学习算法**
### 2.3.1.**原始形式**
  **问题描述**：求解参数$w,b$,使其为以下损失函数极小化问题的解$$\underset{w,b}{min}L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)$$其中，$M$是误分类点的集合。
  **原理**：感知机学习算法是误分类驱动的，具体采用**随机梯度下降算法(stachastic gradient descent)**。首先，任意选取一个超平面$w_0,b_0$,然后用梯度下降法不断地极小化损失函数。极小化过程不是一次使$M$中所有误分类点的梯度下降，而是**一次随机选取一个误分类点使其梯度下降**。
  损失函数$L(w,b)$的梯度：
  $$\begin{array}{cc}
  \nabla_w L(w,b)=-\sum_{x_i\in M}y_ix_i \\
  \nabla_b L(w,b)=-\sum_{x_i\in M}y_i
  \end{array}
  $$   **输入**：训练数据集$T=\{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$ 其中，$x_i\in\mathcal{X}=R^n,y_i\in \mathcal{Y}=\{+1,-1\},i=1,2,\cdots,N$；学习率$\eta$($0 < \eta \leq 0$)
  **输出**：$w,b$; 感知机模型$f(x)=sign(w\cdot x+b)$
  **算法**：
  （1）选取初值$w_0,b_0$
  （2）在训练集中随机选取数据点$(x_i,y_i)$
  （3）如果$y_i(w\cdot x_i+b)\leq 0$:
  $$\begin{array}{cc}
  w\leftarrow w+\eta y_i x_i \\
  b\leftarrow b+\eta y_i
  \end{array}$$ （4）转至（2），直至训练集中没有误分类点
  **直观上理解算法**：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整$w,b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。
### 2.3.2.**对偶形式**
  **基本思想**：将$w,b$表示为实例$x_i$和标记$y_i$的线性组合形式，通过求解其系数而求得$w,b$。
  其中，$w,b$关于$(x_i,y_i)$的增量分别是$\alpha_iy_ix_i，\alpha_iy_i$。这里$\alpha_i=n_i\eta$,假设初始值$w_0,b_0$均为0，则最后学习到的$w,b$可以分别表示为：$$\begin{array}{cc}
  w=\sum_{i=1}^N\alpha_i y_ix_i\\
  \\
  b=\sum_{i=1}^N\alpha_i y_i
  \end{array}$$ **输入**：训练数据集$T=\{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$ 其中，$x_i\in\mathcal{X}=R^n,y_i\in \mathcal{Y}=\{+1,-1\},i=1,2,\cdots,N$；学习率$\eta$($0 < \eta \leq 0$)
  **输出**：$\alpha,b$; 感知机模型$f(x)=sign(\sum_{j=1}^N\alpha_j y_j\cdot x+b)$
  其中，$\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_N)^T$
  **算法**：
  （1）选取初值$w_0,b_0$
  （2）在训练集中随机选取数据点$(x_i,y_i)$
  （3）如果$y_i(\sum_{j=1}^N\alpha_j y_j\cdot x_i+b)\leq 0$:
  $$\begin{array}{cc}
  \alpha_i\leftarrow \alpha_i+\eta\\
  b\leftarrow b+\eta y_i
  \end{array}$$ **注意**：这里参数的迭代，$\alpha_i$的更新是直接计算的，并没用梯度下降；$b$的更新是用梯度下降。
  （4）转至（2），直至训练集中没有误分类点
  **Gram矩阵**：对偶形式中训练实例仅以内积的形式出现（即对偶形式感知机模型中的$x_j\cdot x$），为了方便，预先将训练集中实例间的内积计算出来并以矩阵形式储存，这个矩阵就是Gram矩阵$G=[x_i\cdot y_j]_{N\times N}$
  **理解**：（1）对偶形式将w和b表示成了实例$x_i$和标记$y_i$的线性组合形式，为核函数的引入提供了机会，尤其是像高斯核函数这种泰勒展开式无数项的函数；（2）复杂度只与样本数有关，且因为很多alpha其实是0，所以对优化速度会有一定提升作用。
